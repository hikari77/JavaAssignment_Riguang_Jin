Challenges for single database instance when traffic become heavy
+ limited space, hard to scale up
+ high read/write pressure, high QPS
+ low availability, when machine is down


## Replication
> What.
The copy of an existing db is called replica, and source of the existing db is called master.

> Why.
A single instance db can not handle or even go down if traffic becomes heavy, so how can we decrease the pressure and improve performance. 
One of the ways is separating read and write requests.

> How. (solution of Mysql)
The master node has a binlog that records every activity that master node did before, then replica node asks for binlog and copy that as a relay log into replica node. Then replica node does the exact same operation in the relay log, so try to contain exact same data as master node.

So now, we can direct read traffic to replica nodes and let write requests send to the master node.

But it also introduces another problem.
a read might occur between with a write and commit, so the caller thinks data should exist because write request was sent over but actually might not.

Let’s say insert a new user with ID 100, during insert process, caller wants to read user ID100 because he thinks data was inserted but actually new user has not committed yet, so user ID100 does not exist.

So we need a mechanism that ensures the consistency between master and node. Mysql introduces MGR (Mysql Group Replication) which uses Paxos protocol to ensure data consistency.



Now we have another problem.
What if the master node goes down?

In other words, how can we improve **availability**?

So we need to detect if master goes down and if yes, let one of the replica to become a new master node as soon as possible and keep providing service.
Just like “Do not put all eggs into one bucket”.
We can put machines in different physical locations to improve availability.
One of the great examples is the Availability Zone(AZ) in AWS. They have many data centers located all over the world. many of them might have exact same services.

<br></br>

## Database Sharding 
Split a database into smaller databases in a predictable way.
We can do db partition either horizontally or vertically.
Sharding is the horizontal partition.

If we have a huge table the performance is getting worse and db space is also reaching the limitation of the machine.

We can separate data in a logical way into different machines.
one way is key-based sharding.
we can store a row by doing a hash function and get to know which sharding area this row should be stored in.

Ex. if we want 10 sharding machines, we can module id by 10, so all of the rows can be separated into 10 areas.

<br></br>

## Denormalization

The normalization such as 3NF ensures the table has less redundant data.
But in reality, if we strictly use 3NF, it might hurt performance.
less redundant means we need more tables, more table means when a query comes in we need to join more tables.
So if we want to improve performance then we have to allow a little redundant data to speed up the query, in other words, we can use space to exchange time.
